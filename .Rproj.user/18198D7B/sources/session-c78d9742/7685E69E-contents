# *********************************************************************************
# Lab 9: Ensemble Methods ====
#
# Course Code: BBT4206
# Course Name: Business Intelligence 2
# Strathmore University
# Date: March 2023
#
# Lecturer: Dr Allan Omondi <aomondi@strathmore.edu>
# *********************************************************************************
# Apart from algorithm tuning, you can also combine the predictions of multiple different models together. This is called an "ensemble prediction".

# The 3 most popular ensemble methods for combining the predictions from different models are:
##  (1) Bagging (Bootstrap Aggregation): Building multiple models (typically models of the same type) from different subsamples of the training dataset.
## (2) Boosting: Building multiple models (typically models of the same type) each of which learns to fix the prediction errors of a prior model in the chain.
## (3) Stacking: Building multiple models (typically models of differing types) and a supervised model that learns how to best combine the predictions of the primary models.

# Load packages
library(mlbench)
library(caret)
library(caretEnsemble)
# Load the dataset
data(Ionosphere)
dataset <- Ionosphere

# The second attribute is a constant and has been removed.
dataset <- dataset[,-2]

# The first attribute was a factor (0,1) and has been transformed to be numeric for consistency with all of the other numeric attributes.
dataset$V1 <- as.numeric(as.character(dataset$V1))

# 1. Boosting ====
# Three popular boosting algorithms are:
## 1. AdaBoost.M1
## 2. C5.0
## 3. Stochastic Gradient Boosting

# Example of Boosting Algorithms
trainControl <- trainControl(method="cv", number=5)
seed <- 7
metric <- "Accuracy"

## 1.a. Boosting with C5.0 ----
# C5.0
set.seed(seed)
fit.c50 <- train(Class~., data=dataset, method="C5.0", metric=metric,
                 trControl=trainControl)

## 1.b. Boosting with Stochastic Gradient Boosting ----
set.seed(seed)
fit.gbm <- train(Class~., data=dataset, method="gbm", metric=metric,
                 trControl=trainControl, verbose=FALSE)

## 1.c. Boosting with AdaBoost.M1 ----
set.seed(seed)
# NB: This will take a long time to execute
fit.AdaBoost.M1 <- train(Class~., data=dataset, method="AdaBoost.M1", metric=metric,
               trControl=trainControl)

# summarize results
# NB: Execute the following if you have used the AdaBoost ensemble method above
boostingResults <- resamples(list(c5.0=fit.c50, gbm=fit.gbm, AdaBoost = fit.AdaBoost.M1))

# NB: This is the alternative if you have NOT used the AdaBoost ensemble method above
# boostingResults <- resamples(list(c5.0=fit.c50, gbm=fit.gbm))

summary(boostingResults)
dotplot(boostingResults)

# 2. Bagging ====
# Two popular bagging algorithms are:
## 1. Bagged CART
## 2. Random Forest

# Example of Bagging algorithms
trainControl <- trainControl(method="repeatedcv", number=10, repeats=3)
seed <- 7
metric <- "Accuracy"

## 2.a. Bagged CART ----
set.seed(seed)
fit.treebag <- train(Class~., data=dataset, method="treebag", metric=metric,
                     trControl=trainControl)
## 2.b. Random Forest ----
set.seed(seed)
fit.rf <- train(Class~., data=dataset, method="rf", metric=metric, trControl=trainControl)
# summarize results
baggingResults <- resamples(list(treebag=fit.treebag, rf=fit.rf))
summary(baggingResults)
dotplot(baggingResults)

# 3. Stacking ====
# The "caretEnsemble" package allows you to combine the predictions of multiple caret models.

# Given a list of caret models, the "caretStack()" function (in the "caretEnsemble" package) can be used to specify a higher-order model to learn how to best combine together the predictions of sub-models.

# The "caretList()" function provided by the "caretEnsemble" package can be used to create a list of standard caret models.

# Example of Stacking algorithms
# create submodels
trainControl <- trainControl(method="repeatedcv", number=10, repeats=3,
                             savePredictions=TRUE, classProbs=TRUE)
algorithmList <- c('lda', 'rpart', 'glm', 'knn', 'svmRadial')
set.seed(seed)
models <- caretList(Class~., data=dataset, trControl=trainControl, methodList=algorithmList)
results <- resamples(models)
summary(results)
dotplot(results)


# The predictions made by the sub-models that are combined using stacking should have a low-correlation (for diversity amongst the sub-models, i.e., different sub-models are accurate in different ways). If the predictions for the sub-models were highly correlated (> 0.75) then they would be making the same or very similar predictions most of the time reducing the benefit of combining the predictions.
# correlation between results
modelCor(results)
splom(results)

## 3.a. Stack using glm ----
stackControl <- trainControl(method="repeatedcv", number=10, repeats=3,
                             savePredictions=TRUE, classProbs=TRUE)
set.seed(seed)
stack.glm <- caretStack(models, method="glm", metric="Accuracy", trControl=stackControl)
print(stack.glm)

## 3.b. Stack using random forest ----
set.seed(seed)
stack.rf <- caretStack(models, method="rf", metric="Accuracy", trControl=stackControl)
print(stack.rf)
